---
title: "CLT-based inference - confidence intervals"
author: "Valerie Deligiannis"
date: "3/17/21"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      comment = "#>", highlight = TRUE,
                      fig.align = "center")
```

## Main ideas

- Understand the CLT and how to use the result

- Create confidence intervals for the population mean using a CLT-based
  approach
  
- Create confidence intervals for the population proportion using a CLT-based
  approach
  
# Packages

```{r packages}
library(tidyverse)
library(infer)
```

# Data

In the examples and practice sections, we'll work with a subset of data from 
the General Social Survey.

```{r read_data}
gss_2010 <- read_csv("data/gss_2010.csv")
```

# Notes

Recall that for a population with a well-defined mean $\mu$ and standard 
deviation $\sigma$, these three properties hold for the distribution of sample 
average $\bar{X}$, assuming certain conditions hold:

- The distribution of the sample statistic is nearly normal
- The distribution is centered at the unknown population mean
- The variability of the distribution is inversely proportional to the square
  root of the sample size.

Knowing the distribution of the sample statistic $\bar{X}$ can help us

- estimate a population parameter as point estimate $\pm$ margin of error, where 
  the margin of error is comprised of a measure of how confident we want to be 
  and the sample statistic's variability.

- test for a population parameter by evaluating how likely it is to obtain the
  observed sample statistic when assuming that the null hypothesis is true as 
  this probability will depend on the sampling distribution's variability.

## Normal distribution

If necessary conditions are met, we can also use inference methods based on the 
CLT. Then the CLT tells us that $\bar{X}$ approximately has the distribution 
$N\left(\mu, \sigma/\sqrt{n}\right)$. That is,

$$Z = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim N(0, 1)$$
   
Visualize some normal densities

```{r base_viz, echo=FALSE}
ggbase <- ggplot() +
  xlim(-10, 10) +
  labs(y = "") +
  theme_bw()
```


```{r normal_viz, echo=FALSE}
ggbase +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), 
                color = "red", size = 1.5) +
  stat_function(fun = dnorm, args = list(mean = 2, sd = 2), 
                color = "blue", size = 1.5) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 4), 
                color = "purple", size = 1.5) +
  stat_function(fun = dnorm, args = list(mean = -5, sd = 0.5), 
                color = "grey60", size = 1.5)
```

## t-distribution

While we can (and will) use the CLT result to do inference, in practice, we 
never know the true value of $\sigma$, and so we estimate it
from our data with $s$ (sample standard deviation). The quantity $T$
has a **t-distribution** with $n-1$ *degrees of freedom*:

$$ T = \frac{\bar{X} - \mu}{s/\sqrt{n}} \sim t_{n-1}$$

- The t-distribution is also unimodal and symmetric, and is centered at 0

- It has thicker tails than the normal distribution (to make up for additional 
  variability introduced by using $s$ instead of $\sigma$)
  
```{r t_viz, echo=FALSE}
ggbase +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), 
                color = "grey60", size = 1.5) +
  stat_function(fun = dt, args = list(df = 1), 
                color = "blue", size = 1) +
  stat_function(fun = dt, args = list(df = 5), 
                color = "red", size = 1) +
  stat_function(fun = dt, args = list(df = 10), 
                color = "orange", size = 1) +
  stat_function(fun = dt, args = list(df = 30), 
                color = "violet", size = 1) +
  xlim(-4, 4)
```

What do you notice in the plot?

As the degrees of freedom increases, the distribution becomes a better and 
better approximation for the standard normal distribution. 

## Computing a confidence interval for $\mu$

Recall that in our bootstrap simulation-based approach to creating confidence
intervals, the last step was to calculate the bounds of the `XX%` confidence 
interval as the middle `XX%` of the bootstrap distribution. Rather than work
with the bootstrap distribution, we can work directly with the theoretical
sampling distribution of the sample statistic. We know this from the CLT.

To find cutoffs (quantiles) from the normal and t distributions, we can use 
functions `qnorm()` and `qt()`, respectively.

```{r cutoff_examples}
qnorm(p = 0.975, mean = 0, sd = 1)
qnorm(0.975)

qt(p = 0.975, df = 5)
qt(p = 0.975, df = 10)
qt(p = 0.975, df = 1000)
```

### Example: confidence interval for $\mu$

The GSS asks "After an average work day, about how many 
hours do you have to relax or pursue activities that you enjoy?". Compute a 95%
confidence interval for the mean hours of relaxation time per day after work
using a CLT-based approach.

First, we'll check out our sample data and compute some summary statistics.

```{r summary_stats_example}
hrs_relax_stats <- gss_2010 %>% 
  filter(!is.na(hrsrelax)) %>%
  summarise(x_bar = mean(hrsrelax), 
            s     = sd(hrsrelax), 
            n     = n())

hrs_relax_stats
```

#### Direct calculation via formula

Let's grab these three statistics as vectors to make it easier to compute our
confidence interval.

```{r stats_vectors_example}
n <- hrs_relax_stats$n
x_bar <- hrs_relax_stats$x_bar
s <- hrs_relax_stats$s
```

Our confidence interval formula is given by

$$\mbox{point estimate} \pm t^* \times \mbox{SE},$$

where our point estimate will be the sample mean, $t^*$ is the
cut value from the t-distribution corresponding to the desired confidence level,
and the standard error is a function of the sample standard deviation and sample 
size.

```{r}
ggbase +
  stat_function(fun = dt, args = list(df = n - 1),
                color = "blue", size = 1) +
  xlim(-4,4)
```


$$\bar{x} \pm t^* \times \frac{s}{\sqrt{n}}$$ 

```{r t_crit_example}
(t_star <- qt(p = 0.975, df = n - 1))
```

Why do we have `p = 0.975`?

```{r ci_example}
x_bar + c(-1, 1) * t_star * (s / sqrt(n))
```

How do we interpret this?

We are 95% confident that the interval of (3.53,3.83) captures the true
population mean number of hours people have to relax after work. 

#### Infer

The `infer` package has a function to do these calculations in one
step. Function `t_test()` is a tidier version of the built-in R function
`t.test()`.

```{r t_test}
t_test(gss_2010, response = hrsrelax, conf_level = 0.95) %>%
  select(ends_with("ci"))
```

For now, focus on the last two variables - `lower_ci` and `upper_ci`. Next
time we'll discuss the first four in our lecture on hypothesis testing.

### Assumptions and requirements

What assumptions must we make for this inference procedure to be valid?

1. Assume we have a random sample.

2. $n$ should be large, here $n$ = `r n`

## Practice

The built-in dataset `quakes` gives information on seismic events near Fiji
since 1964. 

(1) Take a random sample of 40 events from `quakes`. You can use 
    `dplyr`'s `slice_sample()`. Save this result as an object named
    `quakes_40`.

```{r practice_1}
quakes_40 <- quakes %>%
  slice_sample(n = 40)
```

(2) Compute some summary statistics from `quakes_40`.

```{r practice_2}
quakes_40 %>%
  summarize(x_bar = mean(depth), s = sd(depth), n = n())
```

(3) Compute a 90% confidence interval for the mean depth of seismic activity
    near Fiji.
    
```{r practice_3}
t_test(quakes_40, response = depth, conf_level = 0.90) %>%
  select(ends_with("ci")) %>% unlist()
```

(4) Give an interpretation of your interval.

We are 90% confident that the mean depth for the seismic activity near Fiji since
1964 is captured by the interval (261.85, 382).

(5) Assume `quakes` consists off all the seismic activity that every occurred
    near Fiji. Does you 90% confidence interval cover the population parameter?

```{r practice_5}
quakes %>%
  summarise(mu = mean(depth))
```

## Computing a confidence interval for $p$

Our sample proportion $\hat{p}$ is the most plausible value of the population
proportion, $p$, so it makes sense to build a confidence interval around this 
point estimate. The standard error provides a guide for how large we should make 
the confidence interval.

The standard error represents the standard deviation of the point estimate, and 
when the Central Limit Theorem conditions are satisfied, the point estimate 
closely follows a normal distribution. The CLT tells 
us that $\hat{p}$ approximately has the distribution 
$N\left(p, \sqrt{\frac{p(1-p)}{n}}\right)$.

To ensure our sample is "large" for proportions, we must verify the
success-failure condition:

1. $n\hat{p} \ge 10$
2. $n(1-\hat{p}) \ge 10$

A confidence interval for $p$ is given by

$$\hat{p} \pm z^* \times \sqrt{\frac{\hat{p}(1-\hat{p})}{n}},$$
where $z^*$ corresponds to the confidence level selected. Since we don't know
$p$ we make a substitution using $\hat{p}$ in our SE.

### Example: confidence interval for $p$

The GSS asks "Are you better off today than you were four years ago?". 
Compute a 95% confidence interval for the proportion of Americans that are
better off today than four years ago. Use a CLT-based approach.

First, we'll check the success-failure condition.

```{r success_failure_check}
gss_2010 %>% 
  count(better)
```

We're also assuming these observations are independent.

Let's compute our 95% confidence interval.

```{r better_ci}
gss_2010 %>% 
  mutate(better = ifelse(better == 1, "better", "worse")) %>% 
  prop_test(response = better, conf_level = 0.95, success = "better") %>% 
  select(ends_with("ci"))
```

## Practice

Redo the above analysis using the confidence interval formula directly, but 
this time create a 90% confidence interval.

```{r formula_ci90_p}
p_hat <- gss_2010 %>%
  count(better) %>%
  mutate(prop = n / sum(n)) %>%
  filter(better == 1) %>%
  pull(prop)

n <- nrow(gss_2010)

z_star <- qnorm(p = 0.95)

p_hat + c(-1,1) * z_star * sqrt(p_hat * (1 - p_hat) / n)
```

## References

1. "Infer - Tidy Statistical Inference". Infer.Netlify.App, 2021, 
   https://infer.netlify.app/index.html.
   
